"""
ì˜¤ë””ì˜¤ ë§ˆìŠ¤í‚¹ ìµœì¢… í†µí•© ë²„ì „
- 8ì¢… ê°œì¸ì •ë³´ íƒì§€: ì´ë¦„, ì „í™”ë²ˆí˜¸, ì´ë©”ì¼, ì£¼ì†Œ, ì£¼ë¯¼ë“±ë¡ë²ˆí˜¸, IPì£¼ì†Œ, ê³„ì¢Œë²ˆí˜¸, ì—¬ê¶Œë²ˆí˜¸
- ì •ê·œì‹ + KoELECTRA í•˜ì´ë¸Œë¦¬ë“œ íƒì§€
- Faster-Whisper STT
"""
import sys
import subprocess
import os
from pathlib import Path

# ==================== í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ ìë™ ì„¤ì¹˜ ====================
def install_and_import(package, import_name=None):
    if import_name is None:
        import_name = package
    try:
        __import__(import_name)
    except ImportError:
        print(f"[ì•Œë¦¼] '{package}' ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤. ì„¤ì¹˜ë¥¼ ì§„í–‰í•©ë‹ˆë‹¤...")
        subprocess.check_call([sys.executable, "-m", "pip", "install", package, "--break-system-packages"])
        print(f"'{package}' ì„¤ì¹˜ ì™„ë£Œ.")

required_packages = [
    ("faster-whisper", "faster_whisper"),
    ("pydub", "pydub"),
    ("librosa", "librosa"),
    ("soundfile", "soundfile"),
    ("torch", "torch"),
    ("transformers", "transformers"),
    ("tqdm", "tqdm"),
]

for package, import_name in required_packages:
    install_and_import(package, import_name)

# ================================================================

import re
import numpy as np
import soundfile as sf
from faster_whisper import WhisperModel
from pydub import AudioSegment
from pydub.generators import Sine
import librosa
import torch
import torch.nn.functional as F
from transformers import AutoTokenizer, AutoModelForTokenClassification
from tqdm import tqdm
from typing import List, Dict, Set
from glob import glob

# ==================== ì„¤ì • ====================
PII_NAMES = {
    'p_nm': 'ì´ë¦„',
    'p_ph': 'ì „í™”ë²ˆí˜¸',
    'p_em': 'ì´ë©”ì¼',
    'p_add': 'ì£¼ì†Œ',
    'p_rrn': 'ì£¼ë¯¼ë“±ë¡ë²ˆí˜¸',
    'p_ip': 'IPì£¼ì†Œ',
    'p_acct': 'ê³„ì¢Œë²ˆí˜¸',
    'p_passport': 'ì—¬ê¶Œë²ˆí˜¸',
}

CONFIDENCE_THRESHOLDS = {
    'p_nm': 0.70,
    'p_ph': 0.75,
    'p_em': 0.75,
    'p_add': 0.80,
    'p_rrn': 0.90,
    'p_ip': 0.75,
    'p_acct': 0.85,
    'p_passport': 0.90,
}

# ==================== ì •ê·œì‹ PII íƒì§€ê¸° (ì™„ì „ ë²„ì „) ====================
class EnhancedRegexPIIDetector:
    """í–¥ìƒëœ ì •ê·œì‹ PII íƒì§€ê¸° - 6ì¢… ì§€ì› (ì´ë¦„, ì „í™”, ì´ë©”ì¼, ì£¼ì†Œ, ì£¼ë¯¼ë²ˆí˜¸, IP)"""
    def __init__(self):
        print("âœ“ ì •ê·œì‹ PII íƒì§€ê¸° ì´ˆê¸°í™” (6ì¢…: ì´ë¦„, ì „í™”ë²ˆí˜¸, ì´ë©”ì¼, ì£¼ì†Œ, ì£¼ë¯¼ë²ˆí˜¸, IP)")

    def detect_phones(self, text: str) -> List[Dict]:
        """ì „í™”ë²ˆí˜¸ íƒì§€"""
        entities = []
        seen = set()

        patterns = [
            (r'01[016789]-\d{3,4}-\d{4}', 'mobile'),
            (r'0(?:2|3[1-3]|4[1-4]|5[1-5]|6[1-4])-\d{3,4}-\d{4}', 'landline'),
            (r'01[016789]\d{7,8}', 'mobile-no-hyphen'),
        ]

        for pattern, phone_type in patterns:
            for match in re.finditer(pattern, text):
                phone = match.group()
                digits = re.sub(r'\D', '', phone)

                if 9 <= len(digits) <= 11 and digits not in seen:
                    seen.add(digits)
                    entities.append({
                        'text': phone,
                        'label': 'p_ph',
                        'start': match.start(),
                        'end': match.end(),
                        'confidence': 1.0,
                        'method': f'regex-{phone_type}'
                    })

        return entities

    def detect_emails(self, text: str) -> List[Dict]:
        """ì´ë©”ì¼ íƒì§€ - OCR ì˜¤ë¥˜ ë³´ì • í¬í•¨"""
        entities = []

        # í‘œì¤€ ì´ë©”ì¼
        standard_pattern = r'[a-zA-Z0-9][a-zA-Z0-9._+-]*@[a-zA-Z0-9][a-zA-Z0-9.-]*\.[a-zA-Z]{2,}'

        for match in re.finditer(standard_pattern, text):
            entities.append({
                'text': match.group(),
                'label': 'p_em',
                'start': match.start(),
                'end': match.end(),
                'confidence': 1.0,
                'method': 'regex-standard'
            })

        # OCR ì˜¤ë¥˜ íŒ¨í„´
        ocr_pattern = r'[a-zA-Z0-9][a-zA-Z0-9._+-]*@[a-zA-Z0-9]+(?:com|net|org|cokr|kr|jp|cn|edu|gov|info)\b'

        for match in re.finditer(ocr_pattern, text):
            if any(e['start'] == match.start() for e in entities):
                continue

            email_text = match.group()
            domain_part = email_text.split('@')[1]

            if '.' not in domain_part:
                known_suffixes = ['com', 'net', 'org', 'cokr', 'kr', 'jp', 'cn', 'edu', 'gov', 'info']

                for suffix in known_suffixes:
                    if domain_part.endswith(suffix):
                        domain_name = domain_part[:-len(suffix)]
                        if len(domain_name) >= 2:
                            local_part = email_text.split('@')[0]
                            
                            if suffix == 'cokr':
                                corrected = f"{local_part}@{domain_name}.co.kr"
                            else:
                                corrected = f"{local_part}@{domain_name}.{suffix}"
                                
                            entities.append({
                                'text': corrected,
                                'label': 'p_em',
                                'start': match.start(),
                                'end': match.end(),
                                'confidence': 0.95,
                                'method': 'regex-ocr-corrected'
                            })
                            break

        return entities

    def detect_addresses(self, text: str) -> List[Dict]:
        """ì£¼ì†Œ íƒì§€"""
        entities = []

        patterns = [
            r'[ê°€-í£]{2,}(?:íŠ¹ë³„ì‹œ|ê´‘ì—­ì‹œ|ë„)\s+[ê°€-í£]{2,}(?:ì‹œ|êµ°|êµ¬)\s+[ê°€-í£]{2,}(?:ë¡œ|ê¸¸)\s+\d+[ê°€-í£0-9\s-]*',
            r'[ê°€-í£]{2,}\s+[ê°€-í£]{2,}(?:ì‹œ|êµ°|êµ¬)\s+[ê°€-í£]{2,}(?:ë¡œ|ê¸¸)?\s*\d*[ê°€-í£0-9\s-]*',
            r'[ê°€-í£]{2,}(?:ì‹œ|êµ°|êµ¬)\s+[ê°€-í£]{2,}(?:ë¡œ|ê¸¸)\s+\d+[ê°€-í£0-9\s-]*',
            r'[ê°€-í£]{2,}êµ¬\s+[ê°€-í£]{2,}(?:ë¡œ|ê¸¸)\s+\d+[ê°€-í£0-9\s-]*',
            r'[ê°€-í£]{2,}(?:ì‹œ|êµ¬)\s+[ê°€-í£]{2,}ë™(?:\s+\d+)?',
        ]

        for pattern in patterns:
            for match in re.finditer(pattern, text):
                address = match.group().strip()

                if self._is_valid_address_structure(address):
                    entities.append({
                        'text': address,
                        'label': 'p_add',
                        'start': match.start(),
                        'end': match.end(),
                        'confidence': 1.0,
                        'method': 'regex'
                    })

        return entities

    def _is_valid_address_structure(self, address: str) -> bool:
        """ì£¼ì†Œ êµ¬ì¡° ê²€ì¦"""
        if len(address) < 8:
            return False

        has_admin = any(kw in address for kw in ['ë„', 'ì‹œ', 'êµ°', 'êµ¬'])
        has_location = any(kw in address for kw in ['ë¡œ', 'ê¸¸', 'ë™'])

        hangul_chars = sum(1 for c in address if 'ê°€' <= c <= 'í£')
        total_chars = len(address.replace(' ', ''))
        hangul_ratio = hangul_chars / total_chars if total_chars > 0 else 0
        is_mostly_hangul = hangul_ratio >= 0.6

        valid_endings = ['ë™', 'ë²ˆê¸¸', 'ë²ˆì§€', 'í˜¸', 'ì¸µ']
        ends_with_number = address[-1].isdigit()
        ends_with_valid_keyword = any(address.endswith(ending) for ending in valid_endings)

        ends_improperly = (
            (address.endswith('êµ¬') or address.endswith('ì‹œ'))
            and not ends_with_valid_keyword
            and not ends_with_number
        )
        ends_properly = not ends_improperly

        return has_admin and has_location and is_mostly_hangul and ends_properly

    def detect_rrn(self, text: str) -> List[Dict]:
        """ì£¼ë¯¼ë“±ë¡ë²ˆí˜¸ íƒì§€"""
        entities = []
        
        pattern1 = r'\d{6}-[1-4]\d{6}'
        pattern2 = r'(?<!\d)\d{13}(?!\d)'
        
        for pattern in [pattern1, pattern2]:
            for match in re.finditer(pattern, text):
                rrn = match.group()
                digits = re.sub(r'\D', '', rrn)
                
                if len(digits) == 13:
                    gender_code = digits[6]
                    if gender_code in '1234':
                        entities.append({
                            'text': rrn,
                            'label': 'p_rrn',
                            'start': match.start(),
                            'end': match.end(),
                            'confidence': 1.0,
                            'method': 'regex'
                        })
        
        return entities

    def detect_ip(self, text: str) -> List[Dict]:
        """IP ì£¼ì†Œ íƒì§€"""
        entities = []
        pattern = r'(?<!\d)\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3}(?!\d)'

        for match in re.finditer(pattern, text):
            ip = match.group()
            try:
                octets = [int(x) for x in ip.split('.')]
                if all(0 <= octet <= 255 for octet in octets):
                    entities.append({
                        'text': ip,
                        'label': 'p_ip',
                        'start': match.start(),
                        'end': match.end(),
                        'confidence': 1.0,
                        'method': 'regex'
                    })
            except:
                pass
        return entities

    def detect_names(self, text: str) -> List[Dict]:
        """ì´ë¦„ íƒì§€ - ë¬¸ë§¥ ê¸°ë°˜"""
        entities = []
        
        context_patterns = [
            r'(?:ì´ë¦„[ì€ëŠ”:]\s*)([ê°€-í£]{2,4})(?=\s|ë‹˜|ì”¨|ì…ë‹ˆë‹¤|$)',
            r'(?:ì„±ëª…[ì€ëŠ”:]\s*)([ê°€-í£]{2,4})(?=\s|ë‹˜|ì”¨|ì…ë‹ˆë‹¤|$)',
            r'([ê°€-í£]{2,4})(?:\s+(?:ê³ ê°|ìƒë‹´ì‚¬|ë‹˜|ì”¨|ì¥êµ°|ëŒ€í‘œ|ë¶€ì¥|ê³¼ì¥|ëŒ€ë¦¬|ì‚¬ì›))',
            r'(?:ê³ ê°\s+)([ê°€-í£]{2,4})(?=\(|ë‹˜|ì”¨|$)',
        ]
        
        common_surnames = [
            'ê¹€', 'ì´', 'ë°•', 'ìµœ', 'ì •', 'ê°•', 'ì¡°', 'ìœ¤', 'ì¥', 'ì„',
            'í•œ', 'ì˜¤', 'ì„œ', 'ì‹ ', 'ê¶Œ', 'í™©', 'ì•ˆ', 'ì†¡', 'ë¥˜', 'í™',
            'ì „', 'ê³ ', 'ë¬¸', 'ì†', 'ë°°', 'ë°±', 'í—ˆ', 'ìœ ', 'ë‚¨', 'ì‹¬',
            'ë…¸', 'í•˜', 'ê³½', 'ì„±', 'ì°¨', 'ì£¼', 'ìš°', 'êµ¬', 'ë°©', 'ê³µ'
        ]
        
        stopwords = {
            'ì´ê²ƒ', 'ê·¸ê²ƒ', 'ì €ê²ƒ', 'ë¬´ì—‡', 'ì–´ë””', 'ëˆ„êµ¬', 'ì–¸ì œ', 'ì–´ë–»ê²Œ',
            'ì´ì œ', 'ê·¸ì œ', 'ì €ì œ', 'ì—¬ê¸°', 'ê±°ê¸°', 'ì €ê¸°',
            'ì´ë¦„', 'ì„±ëª…', 'ê³ ê°', 'ìƒë‹´', 'ë‹´ë‹¹', 'ê´€ë¦¬', 'ì •ë³´',
            'í™•ì¸', 'ì²˜ë¦¬', 'ë“±ë¡', 'ì‚­ì œ', 'ìˆ˜ì •', 'ì¡°íšŒ',
            'ì „í™”', 'ì—°ë½', 'ì „í™”ë²ˆí˜¸', 'ì£¼ì†Œ', 'ì´ë©”ì¼', 'ë©”ì¼',
            'ì„œë²„', 'ì£¼ë¯¼', 'ì£¼ë¯¼ë²ˆí˜¸', 'ê³ ê°ì •ë³´', 'ê³„ì¢Œ',
            'í•˜ë‚˜', 'í•˜ê³ ', 'í•˜ë©´', 'í•©ë‹ˆë‹¤', 'í–ˆìŠµë‹ˆë‹¤', 'í•©ë‹ˆê¹Œ',
            'ì…ë‹ˆë‹¤', 'ìˆìŠµë‹ˆë‹¤', 'ì—†ìŠµë‹ˆë‹¤', 'ë©ë‹ˆë‹¤',
            'ì„œìš¸', 'ë¶€ì‚°', 'ëŒ€êµ¬', 'ì¸ì²œ', 'ê´‘ì£¼', 'ëŒ€ì „', 'ìš¸ì‚°', 'ì„¸ì¢…',
            'ê²½ê¸°', 'ê°•ì›', 'ì¶©ë¶', 'ì¶©ë‚¨', 'ì „ë¶', 'ì „ë‚¨', 'ê²½ë¶', 'ê²½ë‚¨', 'ì œì£¼',
            'ê°•ë‚¨êµ¬', 'ì„œì´ˆêµ¬', 'ì†¡íŒŒêµ¬', 'ê°•ë™êµ¬', 'ì„±ë‚¨ì‹œ', 'ê³ ì–‘ì‹œ',
            'ìš°ë™', 'ì¥êµ°', 'ëŒ€í‘œ', 'ì‚¬ì¥', 'ë¶€ì¥', 'ê³¼ì¥', 'ëŒ€ë¦¬', 'ì‚¬ì›'
        }
        
        for pattern in context_patterns:
            for match in re.finditer(pattern, text):
                if match.groups():
                    name = match.group(1)
                    name_start = match.start(1)
                    name_end = match.end(1)
                else:
                    name = match.group()
                    name_start = match.start()
                    name_end = match.end()
                
                if not (2 <= len(name) <= 4):
                    continue
                
                if name[0] not in common_surnames:
                    continue
                
                if name in stopwords:
                    continue
                
                is_duplicate = any(
                    e['start'] == name_start and e['end'] == name_end and e['label'] == 'p_nm'
                    for e in entities
                )
                
                if not is_duplicate:
                    entities.append({
                        'text': name,
                        'label': 'p_nm',
                        'start': name_start,
                        'end': name_end,
                        'confidence': 0.90,
                        'method': 'regex-korean-name'
                    })
        
        return entities

    def detect_all(self, text: str) -> List[Dict]:
        """6ì¢… PII íƒì§€ (ì •ê·œì‹)"""
        all_entities = []
        
        # ì£¼ë¯¼ë“±ë¡ë²ˆí˜¸ë¥¼ ë¨¼ì € íƒì§€ (ì „í™”ë²ˆí˜¸ ì˜¤íƒ ë°©ì§€)
        rrn_entities = self.detect_rrn(text)
        all_entities.extend(rrn_entities)
        
        rrn_ranges = [(e['start'], e['end']) for e in rrn_entities]
        
        # ì „í™”ë²ˆí˜¸ (ì£¼ë¯¼ë²ˆí˜¸ ë²”ìœ„ ì œì™¸)
        phone_entities = self.detect_phones(text)
        for phone in phone_entities:
            is_inside_rrn = any(
                rrn_start <= phone['start'] < rrn_end or 
                rrn_start < phone['end'] <= rrn_end
                for rrn_start, rrn_end in rrn_ranges
            )
            if not is_inside_rrn:
                all_entities.append(phone)
        
        all_entities.extend(self.detect_emails(text))
        all_entities.extend(self.detect_addresses(text))
        all_entities.extend(self.detect_ip(text))
        all_entities.extend(self.detect_names(text))

        # ì¤‘ë³µ ì œê±°
        seen = set()
        unique = []

        for entity in all_entities:
            key = (entity['start'], entity['end'], entity['label'])
            if key not in seen:
                seen.add(key)
                unique.append(entity)

        unique.sort(key=lambda x: x['start'])
        return unique


# ==================== DL ê¸°ë°˜ PII íƒì§€ê¸° (KoELECTRA) ====================
class KoELECTRAPIIDetector:
    """KoELECTRA NER ëª¨ë¸ - 8ì¢… PII íƒì§€ (íŠ¹íˆ ê³„ì¢Œë²ˆí˜¸, ì—¬ê¶Œë²ˆí˜¸)"""
    def __init__(self, model_path: str, confidence_thresholds: Dict[str, float] = None):
        print(f"  KoELECTRA ëª¨ë¸ ë¡œë“œ ì‹œë„: {model_path}")
        
        # Hugging Face ëª¨ë¸ ì²´í¬ (org/model í˜•ì‹)
        is_hf_model = '/' in model_path and not os.path.exists(model_path)
        
        if not is_hf_model and not os.path.exists(model_path):
            print(f"  âš  ë¡œì»¬ ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {model_path}")
            print(f"  â†’ ì •ê·œì‹ íƒì§€ë§Œ ì‚¬ìš©ë©ë‹ˆë‹¤ (ê³„ì¢Œë²ˆí˜¸, ì—¬ê¶Œë²ˆí˜¸ íƒì§€ ë¶ˆê°€)")
            self.model = None
            return

        try:
            self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
            
            if is_hf_model:
                print(f"  â†’ Hugging Faceì—ì„œ ë‹¤ìš´ë¡œë“œ ì¤‘...")
            
            self.tokenizer = AutoTokenizer.from_pretrained(model_path)
            self.model = AutoModelForTokenClassification.from_pretrained(model_path)
            self.model.to(self.device)
            self.model.eval()

            self.id2label = self.model.config.id2label
            self.label2id = self.model.config.label2id

            self.confidence_thresholds = confidence_thresholds or CONFIDENCE_THRESHOLDS

            print(f"  âœ“ KoELECTRA ëª¨ë¸ ë¡œë“œ ì™„ë£Œ (ë””ë°”ì´ìŠ¤: {self.device})")
            print(f"  â†’ 8ì¢… PII íƒì§€ ê°€ëŠ¥ (ê³„ì¢Œë²ˆí˜¸, ì—¬ê¶Œë²ˆí˜¸ í¬í•¨)")
            print(f"  â†’ ëª¨ë¸ ë ˆì´ë¸”: {list(self.id2label.values())}")
            
        except Exception as e:
            print(f"  âš  ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            print(f"  â†’ ì •ê·œì‹ íƒì§€ë§Œ ì‚¬ìš©ë©ë‹ˆë‹¤ (ê³„ì¢Œë²ˆí˜¸, ì—¬ê¶Œë²ˆí˜¸ íƒì§€ ë¶ˆê°€)")
            self.model = None

    def detect_pii(self, text: str) -> List[Dict]:
        """í…ìŠ¤íŠ¸ì—ì„œ PII íƒì§€"""
        if self.model is None:
            return []
            
        if not text.strip():
            return []

        inputs = self.tokenizer(
            text,
            return_tensors="pt",
            truncation=True,
            max_length=512,
            return_offsets_mapping=True
        )

        input_ids = inputs["input_ids"].to(self.device)
        attention_mask = inputs["attention_mask"].to(self.device)

        with torch.no_grad():
            outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)
            logits = outputs.logits
            probabilities = F.softmax(logits, dim=-1)[0]
            predictions = torch.argmax(probabilities, dim=-1)

        predictions = predictions.cpu().numpy()
        offsets = inputs["offset_mapping"][0].cpu().numpy()

        entities = []
        current_entity = None

        for idx, (pred, offset) in enumerate(zip(predictions, offsets)):
            if offset[0] == 0 and offset[1] == 0:
                continue

            pred_label = self.id2label[pred]
            confidence = probabilities[idx][pred].item()

            if pred_label == 'O':
                if current_entity:
                    entities.append(current_entity)
                    current_entity = None
                continue

            if pred_label.startswith('B-'):
                if current_entity:
                    entities.append(current_entity)

                current_entity = {
                    'label': pred_label[2:],
                    'start': offset[0],
                    'end': offset[1],
                    'confidences': [confidence],
                }

            elif pred_label.startswith('I-'):
                if current_entity and (current_entity['label'] == pred_label[2:]):
                    current_entity['end'] = offset[1]
                    current_entity['confidences'].append(confidence)
                else:
                    if current_entity:
                        entities.append(current_entity)

                    current_entity = {
                        'label': pred_label[2:],
                        'start': offset[0],
                        'end': offset[1],
                        'confidences': [confidence],
                    }

        if current_entity:
            entities.append(current_entity)

        final_results = []
        for ent in entities:
            full_text = text[ent['start']:ent['end']]
            avg_conf = sum(ent['confidences']) / len(ent['confidences'])

            result = {
                'text': full_text,
                'label': ent['label'],
                'start': ent['start'],
                'end': ent['end'],
                'confidence': avg_conf
            }
            final_results.append(result)

        return self.apply_confidence_filter(final_results)

    def apply_confidence_filter(self, entities: List[Dict]) -> List[Dict]:
        """ì‹ ë¢°ë„ ê¸°ë°˜ í•„í„°ë§"""
        if self.model is None:
            return []
            
        filtered = []

        for entity in entities:
            label = entity['label']
            confidence = entity['confidence']
            threshold = self.confidence_thresholds.get(label, 0.75)

            if confidence >= threshold:
                filtered.append(entity)

        return filtered


# ==================== í•˜ì´ë¸Œë¦¬ë“œ PII íƒì§€ê¸° ====================
class HybridPIIDetector:
    """KoELECTRA NER + ì •ê·œì‹ ê¸°ë°˜ í•˜ì´ë¸Œë¦¬ë“œ íƒì§€"""
    def __init__(self, model_path: str, confidence_thresholds: Dict[str, float] = None):
        self.ner_detector = KoELECTRAPIIDetector(model_path, confidence_thresholds)
        self.regex_detector = EnhancedRegexPIIDetector()

    def merge_entities(self, ner_entities: List[Dict], regex_entities: List[Dict]) -> List[Dict]:
        """DL ëª¨ë¸(KoELECTRA)ê³¼ ì •ê·œì‹ ê²°ê³¼ ë³‘í•© (ì¤‘ë³µ ì œê±°)"""
        merged = []

        # 1. ì •ê·œì‹ ê²°ê³¼ëŠ” ë¬´ì¡°ê±´ í¬í•¨ (ìœ„ì¹˜ë‚˜ ë‚´ìš©ì´ ê°™ì•„ë„ regexê°€ ìš°ì„ ì´ê±°ë‚˜, ë”°ë¡œ ì²˜ë¦¬)
        # ë‹¨, regexë¼ë¦¬ ê²¹ì¹˜ëŠ” ê²½ìš°ëŠ” ì•ì„œ regex_detector.detect_allì—ì„œ ì´ë¯¸ ì²˜ë¦¬ë˜ì—ˆë‹¤ê³  ê°€ì •
        merged.extend(regex_entities)

        # 2. KoELECTRA ê²°ê³¼ ì¶”ê°€ (ìœ„ì¹˜ ê²¹ì¹¨ ì²´í¬)
        for ner_entity in ner_entities:
            is_overlapping = False
            for existing in merged:
                if self._is_overlapping(ner_entity, existing):
                    is_overlapping = True
                    break

            if not is_overlapping:
                ner_entity['method'] = 'koelectra'
                merged.append(ner_entity)

        merged.sort(key=lambda x: x['start'])
        return merged

    def _is_overlapping(self, entity1: Dict, entity2: Dict) -> bool:
        """ë‘ ì—”í‹°í‹°ê°€ ê²¹ì¹˜ëŠ”ì§€ í™•ì¸"""
        start1, end1 = entity1['start'], entity1['end']
        start2, end2 = entity2['start'], entity2['end']
        return not (end1 <= start2 or end2 <= start1)

    def _extend_address_entities(self, text: str, entities: List[Dict]) -> List[Dict]:
        """ì£¼ì†Œ ì—”í‹°í‹° í™•ì¥ (íœ´ë¦¬ìŠ¤í‹±: 'ë™', 'í˜¸' ë“± ìƒì„¸ì£¼ì†Œ í¬í•¨ìœ¼ë¡œ í™•ì¥)"""
        extended_entities = []
        
        # í™•ì¥ íŒ¨í„´: (ì•„íŒŒíŠ¸ëª… | ë™/í˜¸/ì¸µ | ìˆ«ì+ë™/í˜¸/ì¸µ | ìˆ«ì)
        extension_pattern = r'^[\s,]*((?:[ê°€-í£a-zA-Z0-9]+(?:íƒ€ìš´|ë¹Œë¼|ë§¨ì…˜|ì•„íŒŒíŠ¸|ì˜¤í”¼ìŠ¤í…”)|[ê°€-í£0-9]+(?:ë™|í˜¸|ì¸µ)|[\d-]+(?:ë™|í˜¸|ì¸µ)?))'

        for entity in entities:
            if entity['label'] != 'p_add':
                extended_entities.append(entity)
                continue
            
            current_end = entity['end']
            
            # ë°˜ë³µì ìœ¼ë¡œ ë’¤ë”°ë¥´ëŠ” ì£¼ì†Œ ìš”ì†Œ í™•ì¸
            while True:
                remaining_text = text[current_end:]
                if not remaining_text:
                    break
                    
                match = re.match(extension_pattern, remaining_text)
                if match:
                    # ë§¤ì¹­ëœ ë¶€ë¶„ë§Œí¼ í™•ì¥
                    matched_str = match.group(0) 
                    
                    new_end = current_end + len(matched_str)
                    
                    # ì—”í‹°í‹° ì—…ë°ì´íŠ¸
                    entity['end'] = new_end
                    entity['text'] = text[entity['start']:new_end]
                    
                    current_end = new_end
                else:
                    break
            
            extended_entities.append(entity)
            
        return extended_entities

    def _extend_short_names(self, text: str, entities: List[Dict]) -> List[Dict]:
        """ì§§ì€ ì´ë¦„ í™•ì¥"""
        STOP_CHARS = {'ì´', 'ê°€', 'ì€', 'ëŠ”', 'ì„', 'ë¥¼', 'ì˜', 'ì—', 'ì™€', 'ê³¼', 'ë¡œ', 'ë„', 'ë§Œ', 'ì”¨', 'ë‹˜', 'êµ°', 'ì–‘', 'ê³¼', 'ì¥'}

        for entity in entities:
            if entity['label'] != 'p_nm':
                continue
                
            name_text = entity['text'].strip()
            # 2ê¸€ì ì´ë¦„ì¸ ê²½ìš°
            if len(name_text) == 2:
                current_end = entity['end']
                if current_end < len(text):
                    next_char = text[current_end]
                    
                    if 'ê°€' <= next_char <= 'í£' and next_char not in STOP_CHARS:
                        entity['end'] += 1
                        entity['text'] = text[entity['start']:entity['end']]
        
        return entities

    def _propagate_known_names(self, text: str, entities: List[Dict], context_names: Set[str] = None) -> List[Dict]:
        """ë¬¸ë§¥ ì „íŒŒ: í™•ì‹¤í•œ ì´ë¦„ì—ì„œ ì´ë¦„ë§Œ ì¶”ì¶œí•˜ì—¬ ê²€ìƒ‰"""
        known_names = set()
        if context_names:
            known_names.update(context_names)

        for entity in entities:
            if entity['label'] == 'p_nm':
                known_names.add(entity['text'])
        
        if not known_names:
            return entities
            
        search_terms = set()
        for name in known_names:
            clean_name = name.strip()
            if len(clean_name) >= 3:
                given_name = clean_name[-2:] 
                search_terms.add(given_name)
            elif len(clean_name) == 2:
                search_terms.add(clean_name)
                
        search_terms = {t for t in search_terms if len(t) >= 2}
        
        if not search_terms:
            return entities
            
        propagated_entities = []
        def is_overlapping(start, end, existing_entities):
            for e in existing_entities:
                if max(start, e['start']) < min(end, e['end']):
                    return True
            return False

        for term in search_terms:
            for match in re.finditer(re.escape(term), text):
                start, end = match.span()
                
                if not is_overlapping(start, end, entities) and not is_overlapping(start, end, propagated_entities):
                    propagated_entities.append({
                        'start': start,
                        'end': end,
                        'text': term,
                        'label': 'p_nm',
                        'confidence': 0.90
                    })

        return entities + propagated_entities

    def _refine_entities(self, entities: List[Dict]) -> List[Dict]:
        """ì—”í‹°í‹° ì •ì œ"""
        refined = []
        
        label_patterns = [
            r'^(?:ì£¼\s*ì†Œ|ê±°\s*ì£¼\s*ì§€|Address|Addr)\s*[:.]?\s*',
            r'^(?:ì„±\s*ëª…|ì´\s*ë¦„|Name)\s*[:.]?\s*',
            r'^(?:ì—°\s*ë½\s*ì²˜|Phone|Mobile|Tel)\s*[:.]?\s*',
            r'^(?:ì´\s*ë©”\s*ì¼|E-?mail)\s*[:.]?\s*',
            r'^(?:ìƒ\s*ë…„\s*ì›”\s*ì¼|Birth)\s*[:.]?\s*',
            r'^(?:ì£¼\s*ë¯¼\s*ë²ˆ\s*í˜¸|RRN)\s*[:.]?\s*'
        ]
        
        for entity in entities:
            text = entity['text']
            label = entity['label']
            
            # 1. ë¼ë²¨ íŠ¸ë¦¬ë°
            for pattern in label_patterns:
                match = re.match(pattern, text, re.IGNORECASE)
                if match:
                    trim_len = len(match.group(0))
                    entity['start'] += trim_len
                    entity['text'] = text[trim_len:]
                    break
            
            # ì£¼ì†Œ í›„ì²˜ë¦¬
            if label == 'p_add':
                suffix_match = re.search(r'(?<=[0-9ê°€-í£])(ì—|ì—ì„œ|ë¡œ|ìœ¼ë¡œ)(\s.*)?$', entity['text'])
                if suffix_match:
                    suffix = suffix_match.group(0)
                    should_trim = False
                    if suffix.startswith('ì—') or suffix.startswith('ì—ì„œ') or suffix.startswith('ìœ¼ë¡œ'):
                        should_trim = True
                    elif suffix.startswith('ë¡œ'):
                        if re.search(r'(ë™|í˜¸|ì¸µ|ë²ˆì§€|[0-9])$', entity['text'][:suffix_match.start()]):
                            should_trim = True
                            
                    if should_trim:
                        trim_len = len(suffix)
                        entity['end'] -= trim_len
                        entity['text'] = entity['text'][:-trim_len]
            
            if not entity['text'].strip():
                continue
                
            # 2. ì˜¤íƒ í•„í„°ë§
            if label == 'p_nm':
                clean_text = entity['text'].strip()
                if len(clean_text) <= 1:
                    continue
                if re.search(r'[0-9!@#$%^&*()_+\-=\[\]{};\':"\\|,.<>/?]', clean_text):
                    continue
            
            refined.append(entity)
            
        return refined

    def detect_pii(self, text: str, context_names: Set[str] = None) -> List[Dict]:
        """í•˜ì´ë¸Œë¦¬ë“œ PII íƒì§€"""
        ner_entities = self.ner_detector.detect_pii(text)
        regex_entities = self.regex_detector.detect_all(text)
        
        merged_entities = self.merge_entities(ner_entities, regex_entities)
        
        merged_entities = self._extend_short_names(text, merged_entities)
        merged_entities = self._extend_address_entities(text, merged_entities)
        merged_entities = self._propagate_known_names(text, merged_entities, context_names)
        merged_entities = self._refine_entities(merged_entities)

        if regex_entities:
            print(f"ì •ê·œì‹ ì¶”ê°€ íƒì§€: {len(regex_entities)}ê°œ")
            for entity in regex_entities:
                print(f"    + {PII_NAMES.get(entity['label'], entity['label'])}: '{entity['text']}'")

        return merged_entities

# ==================== ì˜¤ë””ì˜¤ ì²˜ë¦¬ í•¨ìˆ˜ë“¤ ====================
def initialize_whisper_model(model_size="large-v3", device="auto", num_workers=4):
    """Faster-Whisper ëª¨ë¸ ì´ˆê¸°í™”"""
    if device == "auto":
        device = "cuda" if torch.cuda.is_available() else "cpu"
    
    if device == "cuda":
        compute_type = "float16"
    else:
        compute_type = "int8"
    
    print(f"    Faster-Whisper ëª¨ë¸: {model_size}")
    print(f"    ì¥ì¹˜: {device} | ì—°ì‚° íƒ€ì…: {compute_type}")
    print(f"    ì›Œì»¤ ìˆ˜: {num_workers}")
    
    model = WhisperModel(
        model_size,
        device=device,
        compute_type=compute_type,
        num_workers=num_workers,
        download_root=None
    )
    
    print("    âœ“ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
    return model


def transcribe_audio_with_words(audio_path, model, language='ko'):
    """ì˜¤ë””ì˜¤ â†’ í…ìŠ¤íŠ¸ ë³€í™˜ (ë‹¨ì–´ë³„ íƒ€ì„ìŠ¤íƒ¬í”„ í¬í•¨)"""
    segments, info = model.transcribe(
        audio_path,
        language=language,
        word_timestamps=True,
        vad_filter=True,
        vad_parameters=dict(
            min_silence_duration_ms=500,
            speech_pad_ms=200
        )
    )
    
    all_words = []
    full_text_parts = []
    
    for segment in segments:
        if hasattr(segment, 'words') and segment.words:
            for word_info in segment.words:
                all_words.append({
                    'word': word_info.word.strip(),
                    'start': word_info.start,
                    'end': word_info.end,
                    'probability': word_info.probability
                })
                full_text_parts.append(word_info.word.strip())
    
    full_text = ' '.join(full_text_parts)
    
    return {
        'full_text': full_text,
        'words': all_words,
        'language': info.language,
        'language_probability': info.language_probability,
        'duration': info.duration
    }


def match_pii_timestamps(transcription, pii_entities):
    """PII í…ìŠ¤íŠ¸ë¥¼ ì˜¤ë””ì˜¤ íƒ€ì„ìŠ¤íƒ¬í”„ì™€ ë§¤ì¹­"""
    full_text = transcription['full_text']
    words = transcription['words']
    
    if not words:
        return []
    
    def normalize(s):
        return re.sub(r'\s+', '', s.lower())
    
    normalized_full_text = normalize(full_text)
    
    pii_timestamps = []
    
    for pii in pii_entities:
        pii_text = pii['text']
        pii_normalized = normalize(pii_text)
        
        norm_start = normalized_full_text.find(pii_normalized)
        if norm_start == -1:
            continue
        
        norm_end = norm_start + len(pii_normalized)
        
        # ì›ë³¸ í…ìŠ¤íŠ¸ ìœ„ì¹˜ë¡œ ì—­ë§¤í•‘
        original_start = 0
        norm_pos = 0
        for i, char in enumerate(full_text):
            if not char.isspace():
                if norm_pos == norm_start:
                    original_start = i
                    break
                norm_pos += 1
        
        original_end = original_start
        norm_pos = norm_start
        for i in range(original_start, len(full_text)):
            if not full_text[i].isspace():
                norm_pos += 1
            original_end = i + 1
            if norm_pos >= norm_end:
                break
        
        # ë‹¨ì–´ íƒ€ì„ìŠ¤íƒ¬í”„ ë§¤ì¹­
        char_count = 0
        start_time = None
        end_time = None
        
        for word_info in words:
            word_len = len(word_info['word'])
            word_start = char_count
            word_end = char_count + word_len
            
            if word_start <= original_start < word_end or word_start < original_end <= word_end:
                if start_time is None:
                    start_time = word_info['start']
                end_time = word_info['end']
            
            char_count += word_len + 1
        
        if start_time is not None and end_time is not None:
            pii_timestamps.append({
                'start': max(0, start_time - 0.1),
                'end': end_time + 0.1,
                'type': PII_NAMES.get(pii['label'], pii['label']),
                'text': pii_text
            })
    
    return pii_timestamps


def mask_audio_segments(audio_path, pii_timestamps, tone_freq=1000, fade_duration=50):
    """ì˜¤ë””ì˜¤ì—ì„œ PII êµ¬ê°„ì„ í†¤ìœ¼ë¡œ ë§ˆìŠ¤í‚¹"""
    audio = AudioSegment.from_file(audio_path)
    
    masked_segments = []
    current_time = 0
    
    for pii in pii_timestamps:
        start_ms = int(pii['start'] * 1000)
        end_ms = int(pii['end'] * 1000)
        
        start_ms = max(0, start_ms)
        end_ms = min(len(audio), end_ms)
        
        if start_ms >= end_ms:
            continue
        
        # ë§ˆìŠ¤í‚¹ ì „ êµ¬ê°„
        if current_time < start_ms:
            pre_segment = audio[current_time:start_ms]
            if fade_duration > 0:
                pre_segment = pre_segment.fade_out(fade_duration)
            masked_segments.append(pre_segment)
        
        # ë§ˆìŠ¤í‚¹ êµ¬ê°„
        duration_ms = end_ms - start_ms
        tone = Sine(tone_freq).to_audio_segment(
            duration=duration_ms,
            volume=-20
        )
        
        if fade_duration > 0:
            tone = tone.fade_in(fade_duration).fade_out(fade_duration)
        
        masked_segments.append(tone)
        current_time = end_ms
    
    # ë§ˆì§€ë§‰ êµ¬ê°„
    if current_time < len(audio):
        final_segment = audio[current_time:]
        if fade_duration > 0:
            final_segment = final_segment.fade_in(fade_duration)
        masked_segments.append(final_segment)
    
    final_audio = sum(masked_segments) if masked_segments else audio
    return final_audio


def save_audio(audio, output_path, audio_format="wav"):
    """ì˜¤ë””ì˜¤ ì €ì¥"""
    if not output_path.endswith(f'.{audio_format}'):
        output_path = f"{output_path}.{audio_format}"
    
    audio.export(output_path, format=audio_format)
    return output_path


# ==================== ë©”ì¸ ì²˜ë¦¬ í•¨ìˆ˜ ====================
def process_single_audio(audio_path, output_path, whisper_model, pii_detector, verbose=True):
    """ë‹¨ì¼ ì˜¤ë””ì˜¤ íŒŒì¼ ì²˜ë¦¬"""
    try:
        # 1. STT ë³€í™˜
        if verbose:
            print("    STT ë³€í™˜ ì¤‘...")
        
        transcription = transcribe_audio_with_words(audio_path, whisper_model)
        
        if verbose:
            print(f"    âœ“ ë³€í™˜ ì™„ë£Œ")
            print(f"      í…ìŠ¤íŠ¸: {transcription['full_text'][:100]}{'...' if len(transcription['full_text']) > 100 else ''}")
            print(f"      ì–¸ì–´: {transcription['language']} (í™•ë¥ : {transcription['language_probability']:.2%})")
            print(f"      ë‹¨ì–´ ìˆ˜: {len(transcription['words'])}ê°œ")
        
        # 2. PII íƒì§€
        pii_items = pii_detector.detect_pii(transcription['full_text'])
        
        if not pii_items:
            if verbose:
                print(f"    âš  PII ë¯¸íƒì§€ - ì›ë³¸ ë³µì‚¬")
            import shutil
            shutil.copy2(audio_path, output_path)
            return {
                'success': True,
                'pii_detected': False,
                'output_path': output_path,
                'transcription': transcription['full_text']
            }
        
        if verbose:
            print(f"    âœ“ PII íƒì§€: {len(pii_items)}ê°œ")
            for pii in pii_items:
                method = pii.get('method', 'unknown')
                pii_type = PII_NAMES.get(pii['label'], pii['label'])
                conf = pii.get('confidence', 0)
                print(f"      - [{pii_type}] '{pii['text']}' (ë°©ë²•: {method}, ì‹ ë¢°ë„: {conf:.2f})")
        
        # 3. íƒ€ì„ìŠ¤íƒ¬í”„ ë§¤ì¹­
        pii_timestamps = match_pii_timestamps(transcription, pii_items)
        
        if not pii_timestamps:
            if verbose:
                print(f"    âš  íƒ€ì„ìŠ¤íƒ¬í”„ ë§¤ì¹­ ì‹¤íŒ¨ - ì›ë³¸ ë³µì‚¬")
            import shutil
            shutil.copy2(audio_path, output_path)
            return {
                'success': True,
                'pii_detected': True,
                'timestamp_matched': False,
                'output_path': output_path,
                'transcription': transcription['full_text']
            }
        
        if verbose:
            print(f"    âœ“ íƒ€ì„ìŠ¤íƒ¬í”„ ë§¤ì¹­ ì™„ë£Œ")
            for ts in pii_timestamps:
                print(f"      - {ts['type']}: {ts['start']:.2f}s ~ {ts['end']:.2f}s")
        
        # 4. ë§ˆìŠ¤í‚¹
        if verbose:
            print(f"    ë§ˆìŠ¤í‚¹ ì²˜ë¦¬ ì¤‘...")
        
        masked = mask_audio_segments(audio_path, pii_timestamps)
        
        # 5. ì €ì¥
        final_path = save_audio(masked, output_path)
        
        if verbose:
            print(f"    âœ“ ë§ˆìŠ¤í‚¹ ì™„ë£Œ: {len(pii_timestamps)}ê°œ êµ¬ê°„")
        
        return {
            'success': True,
            'pii_detected': True,
            'timestamp_matched': True,
            'pii_count': len(pii_timestamps),
            'output_path': final_path,
            'transcription': transcription['full_text']
        }
        
    except Exception as e:
        print(f"    âœ— ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
        return {'success': False, 'error': str(e)}


def process_audio_folder(input_folder, output_folder, whisper_model, pii_detector, audio_extensions=['.wav', '.mp3', '.m4a', '.flac']):
    """í´ë” ë‚´ ëª¨ë“  ì˜¤ë””ì˜¤ íŒŒì¼ ë°°ì¹˜ ì²˜ë¦¬"""
    print("\n" + "="*80)
    print(f"í´ë” ë°°ì¹˜ ì²˜ë¦¬: {input_folder}")
    print("="*80)
    
    os.makedirs(output_folder, exist_ok=True)
    
    # ì˜¤ë””ì˜¤ íŒŒì¼ ì°¾ê¸°
    audio_files = []
    for ext in audio_extensions:
        audio_files.extend(glob(os.path.join(input_folder, f"**/*{ext}"), recursive=True))
    
    if not audio_files:
        print(f"âš  ì˜¤ë””ì˜¤ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {input_folder}")
        return
    
    print(f"\nì´ {len(audio_files)}ê°œ íŒŒì¼ ë°œê²¬")
    
    # í†µê³„
    stats = {
        'total': len(audio_files),
        'success': 0,
        'pii_detected': 0,
        'failed': 0
    }
    
    # íŒŒì¼ë³„ ì²˜ë¦¬
    for idx, audio_path in enumerate(audio_files, 1):
        filename = os.path.basename(audio_path)
        print(f"\n{'='*80}")
        print(f"[{idx}/{len(audio_files)}] {filename}")
        print(f"{'='*80}")
        
        output_path = os.path.join(output_folder, f"masked_{filename}")
        
        result = process_single_audio(
            audio_path=audio_path,
            output_path=output_path,
            whisper_model=whisper_model,
            pii_detector=pii_detector,
            verbose=True
        )
        
        if result.get('success', False):
            stats['success'] += 1
            if result.get('pii_detected', False):
                stats['pii_detected'] += 1
        else:
            stats['failed'] += 1
    
    # ìµœì¢… í†µê³„
    print("\n" + "="*80)
    print("ë°°ì¹˜ ì²˜ë¦¬ ì™„ë£Œ!")
    print("="*80)
    print(f"ì´ íŒŒì¼: {stats['total']}ê°œ")
    print(f"ì„±ê³µ: {stats['success']}ê°œ")
    print(f"  - PII íƒì§€ë¨: {stats['pii_detected']}ê°œ")
    print(f"  - PII ì—†ìŒ: {stats['success'] - stats['pii_detected']}ê°œ")
    print(f"ì‹¤íŒ¨: {stats['failed']}ê°œ")
    print(f"\nì¶œë ¥ í´ë”: {os.path.abspath(output_folder)}")


# ==================== ì‹¤í–‰ ====================
if __name__ == "__main__":
    print("\n" + "="*80)
    print("ğŸ¯ PIILOT ì˜¤ë””ì˜¤ ë§ˆìŠ¤í‚¹ ì‹œìŠ¤í…œ")
    print("="*80)
    print("ì§€ì› ê°œì¸ì •ë³´:")
    print("  1. ì´ë¦„ (ì •ê·œì‹)")
    print("  2. ì „í™”ë²ˆí˜¸ (ì •ê·œì‹)")
    print("  3. ì´ë©”ì¼ (ì •ê·œì‹ + OCR ë³´ì •)")
    print("  4. ì£¼ì†Œ (ì •ê·œì‹)")
    print("  5. ì£¼ë¯¼ë“±ë¡ë²ˆí˜¸ (ì •ê·œì‹)")
    print("  6. IPì£¼ì†Œ (ì •ê·œì‹)")
    print("  7. ê³„ì¢Œë²ˆí˜¸ (KoELECTRA ì „ìš©)")
    print("  8. ì—¬ê¶Œë²ˆí˜¸ (KoELECTRA ì „ìš©)")
    print("="*80 + "\n")
    
    # ì„¤ì •
    INPUT_FOLDER = "./generated_audio_dataset"
    OUTPUT_FOLDER = "./masked_audio_output"
    
    # Whisper ì„¤ì •
    WHISPER_MODEL_SIZE = "large-v3"
    DEVICE = "auto"
    NUM_WORKERS = 4
    
    # KoELECTRA ëª¨ë¸ ê²½ë¡œ (Hugging Faceì—ì„œ ìë™ ë‹¤ìš´ë¡œë“œ)
    KOELECTRA_MODEL_PATH = "ParkJunSeong/PIILOT_NER_Model"
    
    # ì…ë ¥ í´ë” í™•ì¸
    if not os.path.exists(INPUT_FOLDER):
        print(f"ì˜¤ë¥˜: ì…ë ¥ í´ë” '{INPUT_FOLDER}'ì´(ê°€) ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
    elif not os.path.isdir(INPUT_FOLDER):
        print(f"ì˜¤ë¥˜: '{INPUT_FOLDER}'ì€(ëŠ”) í´ë”ê°€ ì•„ë‹™ë‹ˆë‹¤.")
    else:
        # Whisper ì´ˆê¸°í™”
        print("Faster-Whisper ëª¨ë¸ ì´ˆê¸°í™”")
        print("-" * 80)
        whisper_model = initialize_whisper_model(
            model_size=WHISPER_MODEL_SIZE,
            device=DEVICE,
            num_workers=NUM_WORKERS
        )
        
        # í•˜ì´ë¸Œë¦¬ë“œ PII íƒì§€ê¸° ì´ˆê¸°í™”
        pii_detector = HybridPIIDetector(
            model_path=KOELECTRA_MODEL_PATH,  # Hugging Faceì—ì„œ ìë™ ë‹¤ìš´ë¡œë“œ
            confidence_thresholds=CONFIDENCE_THRESHOLDS
        )
        
        print("âœ“ ëª¨ë“  ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ\n")
        
        # ë°°ì¹˜ ì²˜ë¦¬ ì‹¤í–‰
        process_audio_folder(
            input_folder=INPUT_FOLDER,
            output_folder=OUTPUT_FOLDER,
            whisper_model=whisper_model,
            pii_detector=pii_detector
        )
